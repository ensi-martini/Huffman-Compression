As humans, we are used to counting in base 10, because we have 10 fingers. We call these decimal digits, the root part DEC stemming from latin the same way decagon does, meaning 10. And as such, there are 10 possibilities
for each character (0-9). Eg, the decimal num 6357:

(6 * 10^3) + (3 * 10^2) + (5 * 10^1) + (7 * 10*0) = 6000 + 300 + 50 + 7 = 6357

In fact, we can actually count in any base we want. Computers store info in bits, short for Binary digITS, which operates in base 2. This means there are 2 possibilities for each character (0 or 1). Eg, the binary number 1011:

(1 * 2^3) + (0 * 2^2) + (1 * 2^1) + (1 * 2^0) = 8 + 0 + 2 + 1 = 11

Interestingly enough, binary and decimal have the same representation for 0 and 1. Everything else is different.

Even more interesting is the fact that we can represent ANY decimal number in binary. The decimal number 1.75 can be mapped in binary as 1.11:

(1 * 2^0) + (1 * 2^-1) + (1 * 2^-2) = 1 + (1/2) + (1/4) = (7/4) = 1.75

Let's take a look at how to convert a decimal to a binary. If we take decimal number 106, we divide by the base we want to put it in (binary, so 2) until we reach a quotient of 0, keeping track of the remainder each step of the way.

(divident, divisor) = (quotient, remainder)

106/2 = (53, 0)
53/2  = (26, 1)
26/2  = (13, 0)
13/2  = ( 6, 1)
6/2   = ( 3, 0)
3/2   = ( 1, 1)
1/2   = ( 0, 1)

We now take these remainders, in reverse order of how we found them (so in this case 1101010), and this is our binary representation. We know this works because we can expand it outwards in binary and return to our original base 10 number:

1101010 = (1 * 2^6) + (1 * 2^5) + (0 * 2^4) + (1 * 2^3) + (0 * 2^2) + (1 * 2^1) + (0 * 2^0)
        = 64 + 32 + 8 + 2
        = 106

This binary representation is a collection of 7 bits (0s and 1s). A byte is collection of 8 bits (0s and 1s). We can model any number from 0-255, after which point we have no more unique combinations of 0s and 1s left to represent anything else. We know that a byte is a collection of 8 bits (which we are within the limit of) and that the max decimal number it can represent is 255 (which we are also within the limit of), so we can now take our binary number and add the appropriate number of 0s to the front of it to make it 8 bits long and throw it into a converter:

bits_to_byte(01101010) -> 106  #DOCSTRING: Return int represented by bits, padded on right.

byte_to_bits(106) -> 01101010 #DOCSTRING: Return the representation of a byte as a string of bits.

So one might ask themselves, what is the point of grouping a binary number, which can be a collection of any length of bits, into a max length of 8? Does this not limit us more than it helps us? The short answer is that, yes, it is an
architechture that has been improved upon and new formats have come into existance, but the origin of computing processors needed to be able to address memory blocks of the same size, so they settled on 8 being the smallest number of bits to be able to hold a significant amount of information. In recent years, we have moved to x16, x32, and now x64 bit processors, but bytes still remain the standard of the smallest form of data.

Nevertheless, it is still more than enough for our purposes. In ASCII, all characters that we may regularly use in a text file can be grouped between the mappings for 32-126, and the rest of the numbers from 0 to 255 represent other less-used symbols. If we make a new text file and write in it 'juan is a bitch', then save and take a look at the file, we will see that it is 15 bytes. This makes sense, because there are 15 characters total (including spaces) and each character has a decimal number mapping, which in turn has a binary number representation, which in turn has a byte representation. Since our string has regular characters, it can be encapsulated within just 15 bytes because each character has a decimal mapping that is no greater than 255, and thus can be represented in 1 byte each.

